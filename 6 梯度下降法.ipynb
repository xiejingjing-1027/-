{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1 什么是梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人的总结：   \n",
    "线性回归：简单线性回归、多元线性回归，其中假设函数是线性的。\n",
    "对于简单线性回归： \n",
    "    * 对损失函数求导得到参数：最小二乘法。\n",
    "    * 梯度下降法，逐渐找到使得损失函数最小的点对应的参数。\n",
    "对于多元线性回归： \n",
    "    * 正规方程法\n",
    "    * 设定合适的损失函数，各个方向趋近使得损失函数最小的点所对应的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不是一个机器学习算法\n",
    "* 是一种基于搜索的最优化的方法（即如何得到损失最小的损失函数）\n",
    "* 作用：最小化一个损失函数（代价函数），我们上一节的方法是直接对损失函数求导，这里是一步一步到达最低点\n",
    "* 梯度上升法：最大化一个效用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **η 学习率**\n",
    "   * 取值决定获得最优解的速度\n",
    "   * 取值不合适时甚至得不到最优解\n",
    "   * 是梯度下降法的一个超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意，在高维平面上，可能有多个极小值，在这种方式下，用梯度下降法找到的局部最优解不一定是全局最优解。\\\n",
    "解决办法：多次运行，随机化初始点\n",
    "* 梯度下降的初始点也是一个超参数\n",
    "* 在本章中在线性回归中使用梯度下降法，损失函数有唯一的最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法的模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.linspace(-1,6,141)\n",
    "plot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y = (plot_x-2.5)**2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即代价函数 H(θ) = （θ-2.5）^2 -1 , 比如简单线性回归，假设函数y = a * x + b ， 固定一个截距b，得到的代价函数可能是这种形式（二维）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(plot_x,plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上述图形的某个横轴求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(theta):\n",
    "    return 2*(theta-2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    return (theta-2.5)**2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # 学习率\n",
    "theta = 0\n",
    "epsilon = 1e-8\n",
    "while True:\n",
    "    gradient = dJ(theta) # 计算梯度（所谓的斜率，单位上升/下降）\n",
    "    last_theta = theta\n",
    "    theta = theta - gradient * eta # 向梯度的相反方向移动，幅度是eta，这一步计算theta移动的距离（向什么方向、步长）\n",
    "    # 到什么程度结束循环？到导数为0的点，这里设定极小值\n",
    "    \n",
    "    if(abs(J(theta)-J(last_theta)) < epsilon):\n",
    "        break\n",
    "\n",
    "print(theta)\n",
    "print(J(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看eta如何决定了移动的方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 \n",
    "theta = 0\n",
    "epsilon = 1e-8\n",
    "theta_history = [theta] # 添加一个列表，记录横轴的theta的值\n",
    "\n",
    "\n",
    "while True:\n",
    "    gradient = dJ(theta) \n",
    "    last_theta = theta\n",
    "    theta = theta - gradient * eta \n",
    "    theta_history.append(theta)\n",
    "    \n",
    "    if(abs(J(theta)-J(last_theta)) < epsilon):\n",
    "        break\n",
    "plt.plot(plot_x,plot_y,'b',)\n",
    "plt.plot(theta_history,J(np.array(theta_history)),color='r',marker='+')\n",
    "#plt.plot(theta_history,J(np.array(theta_history)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装上述代码，查看不同的eta对于梯度下降的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(initial_theta,eta,epsilon=1e-8):\n",
    "    theta = initial_theta\n",
    "    theta_history.append(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = dJ(theta) \n",
    "        last_theta = theta\n",
    "        theta = theta - gradient * eta \n",
    "        theta_history.append(theta)\n",
    "        if(abs(J(theta)-J(last_theta)) < epsilon):\n",
    "            break\n",
    "            \n",
    "def plot_theta_history():\n",
    "    plt.plot(plot_x,plot_y)\n",
    "    plt.plot(np.array(theta_history),J(np.array(theta_history)),color='r',marker='+')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = []\n",
    "gradient_descent(0.,0.01)\n",
    "plot_theta_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = []\n",
    "gradient_descent(0.,0.001)\n",
    "plot_theta_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theta_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = []\n",
    "gradient_descent(0.,0.8)\n",
    "plot_theta_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eta过大的时候"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_history = []\n",
    "# gradient_descent(0.,1.1)\n",
    "# plot_theta_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要重写代价函数/损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    try:\n",
    "        return (theta-2.5)**2-1.\n",
    "    except:\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(initial_theta,eta,n_iters = 1e4, epsilon=1e-8): # 由于可能不会逼近极值点，所以设置最大循环10000，防止进入死循环\n",
    "    theta = initial_theta\n",
    "    theta_history.append(theta)\n",
    "    i_iter = 0 # 设置循环次数\n",
    "    \n",
    "    while i_iter < n_iters:\n",
    "        gradient = dJ(theta) \n",
    "        last_theta = theta\n",
    "        theta = theta - gradient * eta \n",
    "        theta_history.append(theta)\n",
    "        if(abs(J(theta)-J(last_theta)) < epsilon):\n",
    "            break\n",
    "        i_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = []\n",
    "gradient_descent(0.,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theta_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = []\n",
    "gradient_descent(0.,1.1,10)\n",
    "plot_theta_history() # 损失函数越来越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 多元线性回归中的梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述将损失（代价）函数看成一个二次曲线，来模拟梯度下降的过程；这里引入多元线性回归的梯度下降法。 \n",
    "注意之前学过，如果假设函数是y = a + bx，其代价函数就像是一个碗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 假设函数是$\\widehat{y} = \\theta_0 + {\\theta_1}{X_1^{(i)}}+{\\theta_2}{X_2^{(i)}}+{\\theta_3}{X_3^{(i)}}+ ... +{\\theta_n}{X_n^{(i)}} $ \n",
    "\n",
    "* 使得代价函数$J(\\theta)=\\sum_{i=1}^m(y^{(i)}-\\widehat{y}^{(y)})^{2}$取得最小值\n",
    "* 下代入上， 即使得$$J(\\theta)=\\sum_{i=1}^m(y^{(i)}-\\theta_0 - {\\theta_1}{X_1^{(i)}}-{\\theta_2}{X_2^{(i)}} ... -{\\theta_n}{X_n^{(i)}})^{2}$$取得最小值\n",
    "* 对代价函数的各个分量求偏导J\n",
    " * $\\frac{\\partial{J}}{\\partial{\\theta_j}} = \\sum_{i=1}^m 2(y^{(i)}-\\theta_0 - {\\theta_1}{X_1^{(i)}}-{\\theta_2}{X_2^{(i)}} ... -{\\theta_n}{X_n^{(i)}})\\cdot{(-X_j^{(i)})} = \\sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\\theta)\\cdot(-X_j^{(i)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 为了使得梯度 $\\frac{\\partial{J}}{\\partial{\\theta_j}}$和样本容量大小本身没有关系，我们在上个式子乘以$\\frac{1}{m}$，有的教材是（$\\frac{2}{m}$），将代价函数改为$J(\\theta)= \\frac{1}{m}\\sum_{i=1}^m(y^{(i)}-\\widehat{y}^{(y)})^{2}$，此时梯度为\n",
    "$\\frac{\\partial{J}}{\\partial{\\theta_j}} = \\sum_{i=1}^m \\frac{2}{m}\\cdot (y^{(i)}-X_b^{(i)}\\theta)\\cdot(-X_j^{(i)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 其实这里的代价函数就是均方误差 $\\frac{\\partial{J}}{\\partial{\\theta_j}} = MSE(y,\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4 在线性回归模型中使用梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**制造一个虚假的例子，用一元来模拟多元的情况便于可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "x = 2 * np.random.random(size = 100)\n",
    "x\n",
    "y = x * 3. + 4. + np.random.normal(size=100) # 加上符合标准正太分布的噪音\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1,1) # 第二个维度为1 ,100行1列\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,c='r',s=1)\n",
    "plt.xlabel(\"x_value(single feature)\")\n",
    "plt.ylabel(\"y_value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**复习：之前到这一步，我们是怎么做的？** \n",
    "* 假设函数$\\hat{y}= \\theta_0 + \\theta_1\\cdot x$\n",
    "* 代价函数 $\\sum_{i=1}^{m}(y^{(i)}-\\hat{y}^{(i)})^{2}$,使其最小，直接求导，使导数==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**现在我们用梯度下降法，首先设置假设函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意是矩阵运算： \n",
    "* theta是(n+1)*1矩阵，第一个数字是截距，剩下的是斜率参数\n",
    "* X_b是m个样本n个特征组成的m*(n+1)的矩阵,第一列加上一列1\n",
    "* y是样本的值（监督学习），是m*1的矩阵\n",
    "* 返回的是代价函数的一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta,X_b,y):\n",
    "    try:\n",
    "        return np.sum((y-X_b.dot(theta))**2) / len(X_b)\n",
    "    except:\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(theta,X_b,y):\n",
    "    res = np.empty(len(theta)) # n个维度，将求出n个导数\n",
    "    res[0] = np.sum(X_b.dot(theta)-y)\n",
    "    for i in range(1,len(theta)):\n",
    "        res[i] = (X_b.dot(theta)-y).dot(X_b[:,i])\n",
    "    return res * 2 / len(X_b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_b,y,initial_theta,eta,n_iters = 1e4, epsilon=1e-8):\n",
    "    theta = initial_theta\n",
    "    i_iter = 0 # 设置循环次数\n",
    "    \n",
    "    while i_iter < n_iters:\n",
    "        gradient = dJ(theta,X_b,y) \n",
    "        last_theta = theta\n",
    "        theta = theta - gradient * eta \n",
    "        theta_history.append(theta)\n",
    "        if(abs(J(theta,X_b,y)-J(last_theta,X_b,y)) < epsilon):\n",
    "            break\n",
    "        i_iter += 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.hstack((np.ones((len(x),1)),x.reshape(-1,1)))\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "eta = 0.01\n",
    "\n",
    "theta = gradient_descent(X_b,y,initial_theta,eta)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Linear Regression包中实现自己写的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit_gd(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.interception_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-5 线性回归梯度下降法的向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**向量计算**  \n",
    "注意在numpy中一维向量不区分横竖，会自动计算一维向量的横竖，（这里我们把一维计算结果默认为列），这里为了严谨区分了横竖，其实无论写不写$^T$，结果都一样\n",
    "$ DJ = \\frac{DJ}{D\\theta} = \\vec{\\theta} = \\frac{2}{m}\\cdot (X_b\\theta-y)^T\\cdot X_b$，  \n",
    "或者 $ DJ = \\frac{DJ}{D\\theta} = \\vec{\\theta} = \\frac{2}{m}\\cdot X_b^T \\cdot (X_b\\theta-y)$ 变成横向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点乘运算会自动判别一维向量是横向量还是纵向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(range(16)).reshape(4,-1)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "boston.keys()\n",
    "boston.data.shape\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = X[y<50.0]\n",
    "y = y[y<50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,seed = 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regl = LinearRegression()\n",
    "%time lin_regl.fit_normal(X_train,y_train)\n",
    "lin_regl.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2 = LinearRegression()\n",
    "%time lin_reg2.fit_gd(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因：真实的数据集：每一个特征的数据规模完全不同，eta即使设置得很小，但是步长还是太大，结果不收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2 = LinearRegression()\n",
    "%time lin_reg2.fit_gd(X_train,y_train,eta=0.000001)\n",
    "lin_reg2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么成功率这么低？可能是eta太小了，在有限的次数下难以收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2 = LinearRegression()\n",
    "%time lin_reg2.fit_gd(X_train,y_train,eta=0.000001,n_iters=1e6)\n",
    "lin_reg2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决方式：数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_standard = standardScaler.transform(X_train)\n",
    "X_train_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg3 = LinearRegression()\n",
    "lin_reg3.fit_gd(X_train_standard,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_standard = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg3._theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg3.score(X_test_standard,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法相对于正规方程法的优势在于：特征数越多，梯度下降法相对耗时短。（矩阵运算慢）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-6 随机梯度下降法 Stochastic  Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**考虑一种情况，给定的样本数量极其庞大**，为了方便，在求偏导的时候，只对其中的一个样本做计算，即 $$ = \\frac{2}{m}\\left[ \\begin{matrix} \\sum_{i=1}^{m} (X_b^{(i)}\\theta-y^{(i)})\\cdot X_0^{(i)} \\\\ \\sum_{i=1}^{m} (X_b^{(i)}\\theta-y^{(i)})\\cdot X_1^{(i)} \\\\ \\sum_{i=1}^{m} (X_b^{(i)}\\theta-y^{(i)})\\cdot X_2^{(i)}\\\\ \\sum_{i=1}^{m} (X_b^{(i)}\\theta-y^{(i)}) \\cdot X_3^{(i)} \\\\ \\cdots \\\\ \\sum_{i=1}^{m} (X_b^{(i)}\\theta-y^{(i)})\\cdot X_n^{(i)}\\end{matrix} \\right] $$  $$=2 \\cdot \\left[ \\begin{matrix} \\ (X_b^{(i)}\\theta-y^{(i)})\\cdot X_0^{(i)} \\\\ (X_b^{(i)}\\theta-y^{(i)})\\cdot X_1^{(i)} \\\\  (X_b^{(i)}\\theta-y^{(i)})\\cdot X_2^{(i)}\\\\  (X_b^{(i)}\\theta-y^{(i)})\\cdot X_3^{(i)} \\\\ \\cdots \\\\  (X_b^{(i)}\\theta-y^{(i)}).\\cdot X_n^{(i)}\\end{matrix} \\right]$$$$=2\\cdot(X_b^{(i)})^T\\cdot(X_b^{(i)}\\theta-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照之前的梯度下降，会沿着损失函数最小值的方向向下移动，但是那样做在样本值很大的情况下，会导致每个方向都要计算，计算量很大；这样选择一个样本计算，可以下降的道路比较曲折，但是方向是正确的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**关于学习率要注意以下几点** \n",
    "* 学习率的取值非常重要。由于随机过程不好，容易跳出最小值所在的位置，所以实际上学习率是逐渐递减的。设计一个关于循环次数的函数。\n",
    "* $\\eta = \\frac{1}{i_{-}iters}$\n",
    "* 当循环次数小的时候，$\\eta$下降的速度实在太快了，所以给分母加上一个数$\\eta = \\frac{1}{i_{-}iters+b}$，b可以取50\n",
    "* 分子的位置，取一个常数，使之更灵活，比如a\n",
    "* 这里的a,b就是超参数\n",
    "* 这是搜索领域的模拟退火的思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100000\n",
    "x = np.random.normal(size=m)\n",
    "X = x.reshape(-1,1)\n",
    "y = 4. * x + 3. + np.random.normal(0,3,size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta,X_b,y):\n",
    "    try:\n",
    "        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def dJ(theta,X_b,y):\n",
    "    return X_b.T.dot(X_b.dot(theta)-y) * 2. / len(y)\n",
    "\n",
    "def gradient_descent(X_b,y,initial_theta,eta,n_iters=1e4,epsilon=1e-8):\n",
    "    theta = initial_theta\n",
    "    cur_iter = 0\n",
    "    while cur_iter < n_iters:\n",
    "        gradient = dJ(theta,X_b,y)\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * gradient\n",
    "        if abs(J(theta,X_b,y)-J(last_theta,X_b,y))<epsilon:\n",
    "            break\n",
    "        cur_iter += 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_b = np.hstack([np.ones((len(X),1)),X])\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "eta = 0.01\n",
    "theta = gradient_descent(X_b,y,initial_theta,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用随机梯度下降法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta,X_b,y):\n",
    "    try:\n",
    "        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)\n",
    "    except:\n",
    "        return float('inf')\n",
    "    \n",
    "# def dJ(theta,X_b,y):\n",
    "#     return X_b.T.dot(X_b.dot(theta)-y) * 2. / len(y)\n",
    "\n",
    "#求梯度的方向\n",
    "def dJ_sgd(theta,X_b_i,y_i):\n",
    "    return X_b_i.T.dot(X_b_i.dot(theta)-y_i) * 2. \n",
    "\n",
    "def sgd(X_b,y,initial_theta,n_iters):\n",
    "    t0 = 5\n",
    "    t1 = 50\n",
    "    \n",
    "    def learning_rate(t):\n",
    "        return t0 / (t + t1)\n",
    "    theta = initial_theta\n",
    "    \n",
    "    for cur_iter in range(n_iters):\n",
    "        rand_i = np.random.randint(len(X_b))\n",
    "        gradient = dJ_sgd(theta,X_b[rand_i],y[rand_i])\n",
    "        theta = theta - learning_rate(cur_iter)*gradient\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_b = np.hstack([np.ones((len(X),1)),X])\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "theta = sgd(X_b,y,initial_theta,n_iters = len(X_b)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-7 scikit-learn中的随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将随机梯度下降法封装在自己的函数中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100000\n",
    "\n",
    "x = np.random.normal(size=m)\n",
    "X = x.reshape(-1,1)\n",
    "y = 4. *x + 3. + np.random.normal(0,3,size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playML.LinearRegression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit_sgd(X,y,n_iters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.97909507])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9894919785949567"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.interception_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用真实的数据，使用自己的sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "boston.keys()\n",
    "boston.data.shape\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = X[y<50.0]\n",
    "y = y[y<50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,seed = 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X_train)\n",
    "x_train_standard = standardScaler.transform(X_train)\n",
    "x_test_standard = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7857275413602651"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playML.LinearRegression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "%time lin_reg.fit_sgd(x_train_standard,y_train,n_iters=2)\n",
    "lin_reg.score(x_test_standard,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当观察的样本整体的数量增加，结果也更加准确了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 148 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.808560757055621"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playML.LinearRegression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "%time lin_reg.fit_sgd(x_train_standard,y_train,n_iters=50)\n",
    "lin_reg.score(x_test_standard,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在scikit-learn中的SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sgd_reg.fit(x_train_standard,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8123287969388414"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.score(x_test_standard,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sgd_reg = SGDRegressor()可以传入参数：样本整体浏览多少次，默认是5；如sgd_reg = SGDRegressor(n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-8 如何确定梯度下降法的准确性 调试梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-9 有关梯度下降法的更多的深入讨论"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
